<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="keywords" content="Julia, Explainable Artificial Intelligence, Counterfactual Explanations, Algorithmic Recourse">

<title>Explaining Black-Box Models through Counterfactuals</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="paper_files/libs/clipboard/clipboard.min.js"></script>
<script src="paper_files/libs/quarto-html/quarto.js"></script>
<script src="paper_files/libs/quarto-html/popper.min.js"></script>
<script src="paper_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="paper_files/libs/quarto-html/anchor.min.js"></script>
<link href="paper_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="paper_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="paper_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="paper_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="paper_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../_extensions/juliacon-proceedings/styles.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="paper.pdf"><i class="bi bi-file-pdf"></i>PDF (juliacon-proceedings)</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Explaining Black-Box Models through Counterfactuals</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Patrick Altmeyer <a href="https://orcid.org/0000-0003-4726-8613" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Delft University of Technology
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Arie van Deursen </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Delft University of Technology
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Cynthia C. S. Liem </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Delft University of Technology
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    <p>We present <code>CounterfactualExplanations.jl</code>: a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box models in Julia. CE explain how inputs into a model need to change to yield specific model predictions. Explanations that involve realistic and actionable changes can be used to provide AR: a set of proposed actions for individuals to change an undesirable outcome for the better. In this article, we discuss the usefulness of CE for Explainable Artificial Intelligence and demonstrate the functionality of our package. The package is straightforward to use and designed with a focus on customization and extensibility. We envision it to one day be the go-to place for explaining arbitrary predictive models in Julia through a diverse suite of counterfactual generators.</p>
  </div>
</div>

</header>

<section id="sec-intro" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Machine Learning models like Deep Neural Networks have become so complex and opaque over recent years that they are generally considered black-box systems. This lack of transparency exacerbates several other problems typically associated with these models: they tend to be unstable <span class="citation" data-cites="goodfellow2014explaining">(<a href="#ref-goodfellow2014explaining" role="doc-biblioref">Goodfellow, Shlens, and Szegedy 2014</a>)</span>, encode existing biases <span class="citation" data-cites="buolamwini2018gender">(<a href="#ref-buolamwini2018gender" role="doc-biblioref">Buolamwini and Gebru 2018</a>)</span> and learn representations that are surprising or even counter-intuitive from a human perspective <span class="citation" data-cites="buolamwini2018gender">(<a href="#ref-buolamwini2018gender" role="doc-biblioref">Buolamwini and Gebru 2018</a>)</span>. Nonetheless, they often form the basis for data-driven decision-making systems in real-world applications.</p>
<p>As others have pointed out, this scenario gives rise to an undesirable principal-agent problem involving a group of principals—i.e.&nbsp;human stakeholders—that fail to understand the behaviour of their agent—i.e.&nbsp;the black-box system <span class="citation" data-cites="borch2022machine">(<a href="#ref-borch2022machine" role="doc-biblioref">Borch 2022</a>)</span>. The group of principals may include programmers, product managers and other decision-makers who develop and operate the system as well as those individuals ultimately subject to the decisions made by the system. In practice, decisions made by black-box systems are typically left unchallenged since the group of principals cannot scrutinize them:</p>
<blockquote class="blockquote">
<p>“You cannot appeal to (algorithms). They do not listen. Nor do they bend.” <span class="citation" data-cites="oneil2016weapons">(<a href="#ref-oneil2016weapons" role="doc-biblioref">O’Neil 2016</a>)</span></p>
</blockquote>
<p>In light of all this, a quickly growing body of literature on Explainable Artificial Intelligence (XAI) has emerged. Counterfactual Explanations fall into this broad category. They can help human stakeholders make sense of the systems they develop, use or endure: they explain how inputs into a system need to change for it to produce different decisions. Explainability benefits internal as well as external quality assurance. Explanations that involve plausible and actionable changes can be used for Algorithmic Recourse (AR): they offer the group of principals a way to not only understand their agent’s behaviour but also adjust or react to it.</p>
<p>The availability of open-source software to explain black-box models through counterfactuals is still limited. Through the work presented here, we aim to close that gap and thereby contribute to broader community efforts towards XAI. We envision this package to one day be the go-to place for Counterfactual Explanations in Julia. Thanks to Julia’s unique support for interoperability with foreign programming languages we believe that this library may also benefit the broader machine learning and data science community.</p>
<p>Our package provides a simple and intuitive interface to generate CE for many standard classification models trained in Julia, as well as in Python and R. It comes with detailed documentation involving various illustrative example datasets, models and counterfactual generators for binary and multi-class prediction tasks. A carefully designed package architecture allows for a seamless extension of the package functionality through custom generators and models.</p>
<p>The remainder of this article is structured as follows: <a href="#sec-related">Section&nbsp;2</a> presents related work on XAI as well as a brief overview of the methodological framework underlying CE. <a href="#sec-arch">Section&nbsp;3</a> introduces the Julia package and its high-level architecture. <a href="#sec-use">Section&nbsp;4</a> presents several basic and advanced usage examples. In <a href="#sec-custom">Section&nbsp;5</a> we demonstrate how the package functionality can be customized and extended. To illustrate its practical usability, we explore examples involving real-world data in <a href="#sec-emp">Section&nbsp;6</a>. Finally, we also discuss the current limitations of our package, as well as its future outlook in <a href="#sec-outlook">Section&nbsp;7</a>. <a href="#sec-conclude">Section&nbsp;8</a> concludes.</p>
</section>
<section id="sec-related" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Background and related work</h1>
<p>In this section, we first briefly introduce the broad field of Explainable AI, before narrowing it down to Counterfactual Explanations. We introduce the methodological framework and finally point to existing open-source software.</p>
<section id="literature-on-explainable-ai" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="literature-on-explainable-ai"><span class="header-section-number">2.1</span> Literature on Explainable AI</h2>
<p>The field of XAI is still relatively young and made up of a variety of subdomains, definitions, concepts and taxonomies. Covering all of these is beyond the scope of this article, so we will focus only on high-level concepts. The following literature surveys provide more detail: Arrieta et al.&nbsp;(2020) provide a broad overview of XAI <span class="citation" data-cites="arrieta2020explainable">(<a href="#ref-arrieta2020explainable" role="doc-biblioref">Arrieta et al. 2020</a>)</span>; Fan et al.&nbsp;(2020) focus on explainability in the context of deep learning <span class="citation" data-cites="fan2020interpretability">(<a href="#ref-fan2020interpretability" role="doc-biblioref">Fan, Xiong, and Wang 2020</a>)</span>; and finally, Karimi et al.&nbsp;(2020) <span class="citation" data-cites="karimi2020survey">(<a href="#ref-karimi2020survey" role="doc-biblioref">Karimi, Barthe, et al. 2020</a>)</span> and Verma et al.&nbsp;(2020) <span class="citation" data-cites="verma2020counterfactual">Verma, Dickerson, and Hines (<a href="#ref-verma2020counterfactual" role="doc-biblioref">2020</a>)</span> offer detailed reviews of the literature on Counterfactual Explanations and Algorithmic Recourse (see also <span class="citation" data-cites="molnar2020interpretable">Molnar (<a href="#ref-molnar2020interpretable" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="varshney2022trustworthy">Varshney (<a href="#ref-varshney2022trustworthy" role="doc-biblioref">2022</a>)</span>). Miller (2019) explicitly discusses the concept of explainability from the perspective of a social scientist <span class="citation" data-cites="miller2019explanation">(<a href="#ref-miller2019explanation" role="doc-biblioref">Miller 2019</a>)</span>.</p>
<p>The first broad distinction we want to make here is between <strong>Interpretable</strong> and <strong>Explainable</strong> AI. These terms are often used interchangeably, but this can lead to confusion. We find the distinction made in <span class="citation" data-cites="rudin2019stop">Rudin (<a href="#ref-rudin2019stop" role="doc-biblioref">2019</a>)</span> useful: Interpretable AI involves models that are inherently interpretable and transparent such as general additive models (GAM), decision trees and rule-based models; Explainable AI involves models that are not inherently interpretable but require additional tools to be explainable to humans. Examples of the latter include Ensembles, Support Vector Machines and Deep Neural Networks. Some would argue that we best avoid the second category of models altogether and instead focus solely on interpretable AI <span class="citation" data-cites="rudin2019stop">Rudin (<a href="#ref-rudin2019stop" role="doc-biblioref">2019</a>)</span>. While we agree that initial efforts should always be geared towards interpretable models, avoiding black boxes altogether would entail missed opportunities and anyway is probably not very realistic at this point. For that reason, we expect the need for XAI to persist in the medium term. Explainable AI can further be broadly divided into <strong>global</strong> and <strong>local</strong> explainability: the former is concerned with explaining the average behaviour of a model, while the latter involves explanations for individual predictions <span class="citation" data-cites="molnar2020interpretable">(<a href="#ref-molnar2020interpretable" role="doc-biblioref">Molnar 2020</a>)</span>. Tools for global explainability include partial dependence plots (PDP), which involve the computation of marginal effects through Monte Carlo, and global surrogates. A surrogate model is an interpretable model that is trained to explain the predictions of a black-box model.</p>
<p>Counterfactual Explanations fall into the category of local methods: they explain how individual predictions change in response to individual feature perturbations. Among the most popular alternatives to Counterfactual Explanations are local surrogate explainers including Local Interpretable Model-agnostic Explanations (LIME) and Shapley additive explanations (SHAP). Since explanations produced by LIME and SHAP typically involve simple feature importance plots, they arguably rely on reasonably interpretable features at the very least. Contrary to Counterfactual Explanations, for example, it is not obvious how to apply LIME and SHAP to high-dimensional image data. Nonetheless, local surrogate explainers are among the most widely used XAI tools today, potentially because they are easy to interpret and implemented in popular programming languages. Proponents of surrogate explainers also commonly mention that there is a straightforward way to assess their reliability: a surrogate model that generates predictions in line with those produced by the black-box model is said to have high <strong>fidelity</strong> and therefore considered reliable. As intuitive as this notion may be, it also points to an obvious shortfall of surrogate explainers: even a high-fidelity surrogate model that produces the same predictions as the black-box model 99 per cent of the time is useless and potentially misleading for every 1 out of 100 individual predictions.</p>
<p>A recent study has shown that even experienced data scientists tend to put too much trust in explanations produced by LIME and SHAP <span class="citation" data-cites="kaur2020interpreting">(<a href="#ref-kaur2020interpreting" role="doc-biblioref">Kaur et al. 2020</a>)</span>. Another recent work has shown that both methods can be easily fooled: they depend on random input perturbations, a property that can be abused by adverse agents to essentially whitewash strongly biased black-box models <span class="citation" data-cites="slack2020fooling">(<a href="#ref-slack2020fooling" role="doc-biblioref">Slack et al. 2020</a>)</span>. In related work, the same authors find that while gradient-based Counterfactual Explanations can also be manipulated, there is a straightforward way to protect against this in practice <span class="citation" data-cites="slack2021counterfactual">(<a href="#ref-slack2021counterfactual" role="doc-biblioref">Slack et al. 2021</a>)</span>. In the context of quality assessment, it is also worth noting that—contrary to surrogate explainers—CE always achieve full fidelity by construction: counterfactuals are searched with respect to the black-box classifier, not some proxy for it. That being said, CE should also be used with care and research around them is still in its early stages.</p>
</section>
<section id="sec-method" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-method"><span class="header-section-number">2.2</span> A framework for Counterfactual Explanations</h2>
<p>Counterfactual search involves feature perturbations: we are interested in understanding how we need to change individual attributes in order to change the model output to a desired value or label <span class="citation" data-cites="molnar2020interpretable">(<a href="#ref-molnar2020interpretable" role="doc-biblioref">Molnar 2020</a>)</span>. Typically the underlying methodology is presented in the context of binary classification: <span class="math inline">\(M: \mathcal{X} \mapsto \mathcal{Y}\)</span> where <span class="math inline">\(\mathcal{X}\subset\mathbb{R}^D\)</span> and <span class="math inline">\(\mathcal{Y}=\{0,1\}\)</span>. Further, let <span class="math inline">\(t=1\)</span> be the target class and let <span class="math inline">\(x\)</span> denote the factual feature vector of some individual sample outside of the target class, so <span class="math inline">\(y=M(x)=0\)</span>. We follow this convention here, though it should be noted that the ideas presented here also carry over to multi-class problems and regression <span class="citation" data-cites="molnar2020interpretable">(<a href="#ref-molnar2020interpretable" role="doc-biblioref">Molnar 2020</a>)</span>.</p>
<p>The counterfactual search objective originally proposed by <span class="citation" data-cites="wachter2017counterfactual">Wachter, Mittelstadt, and Russell (<a href="#ref-wachter2017counterfactual" role="doc-biblioref">2017</a>)</span> is as follows</p>
<p><span id="eq-obj"><span class="math display">\[
\min_{x^\prime \in \mathcal{X}} h(x^\prime) \ \ \ \mbox{s. t.} \ \ \ M(x^\prime) = t
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(h(\cdot)\)</span> quantifies how complex or costly it is to go from the factual <span class="math inline">\(x\)</span> to the counterfactual <span class="math inline">\(x^\prime\)</span>. To simplify things we can restate this constrained objective as the following unconstrained and differentiable problem:</p>
<p><span id="eq-solution"><span class="math display">\[
x^\prime = \arg \min_{x^\prime}  \ell(M(x^\prime),t) + \lambda h(x^\prime)
\tag{2}\]</span></span></p>
<p>Here <span class="math inline">\(\ell\)</span> denotes some loss function targeting the deviation between the target label and the predicted label and <span class="math inline">\(\lambda\)</span> governs the strength of the complexity penalty. Provided we have gradient access for the black-box model <span class="math inline">\(M\)</span> the solution to this problem can be found through gradient descent. This generic framework lays the foundation for most state-of-the-art approaches to counterfactual search and is also used as the baseline approach in our package. The hyperparameter <span class="math inline">\(\lambda\)</span> is typically tuned through grid search or in some sense pre-determined by the nature of the problem. Conventional choices for <span class="math inline">\(\ell\)</span> include margin-based losses like cross-entropy loss and hinge loss. It is worth pointing out that the loss function is typically computed with respect to logits rather than predicted probabilities, a convention that we have chosen to follow.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Numerous extensions to this simple approach have been developed since CE were first proposed in 2017 (see <span class="citation" data-cites="verma2020counterfactual">Verma, Dickerson, and Hines (<a href="#ref-verma2020counterfactual" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="karimi2020survey">Karimi, Barthe, et al. (<a href="#ref-karimi2020survey" role="doc-biblioref">2020</a>)</span> for surveys). The various approaches largely differ in that they use different flavours of search objective defined in <a href="#eq-solution">Equation&nbsp;2</a>. Different penalties are often used to address many of the desirable properties of effective CE that have been set out. These desiderata include: <strong>proximity</strong> — the distance between factual and counterfactual features should be small <span class="citation" data-cites="wachter2017counterfactual">(<a href="#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span>; <strong>actionability</strong> — the proposed recourse should be actionable <span class="citation" data-cites="ustun2019actionable poyiadzi2020face">(<a href="#ref-ustun2019actionable" role="doc-biblioref">Ustun, Spangher, and Liu 2019</a>; <a href="#ref-poyiadzi2020face" role="doc-biblioref">Poyiadzi et al. 2020</a>)</span>; <strong>plausibility</strong> — the counterfactual explanation should be plausible to a human <span class="citation" data-cites="joshi2019realistic schut2021generating">(<a href="#ref-joshi2019realistic" role="doc-biblioref">Joshi et al. 2019</a>; <a href="#ref-schut2021generating" role="doc-biblioref">Schut et al. 2021</a>)</span>; <strong>sparsity</strong> — the counterfactual explanation should involve as few individual feature changes as possible <span class="citation" data-cites="schut2021generating">(<a href="#ref-schut2021generating" role="doc-biblioref">Schut et al. 2021</a>)</span>; <strong>robustness</strong> — the counterfactual explanation should be robust to domain and model shifts <span class="citation" data-cites="upadhyay2021robust">(<a href="#ref-upadhyay2021robust" role="doc-biblioref">Upadhyay, Joshi, and Lakkaraju 2021</a>)</span>; <strong>diversity</strong> — ideally multiple diverse counterfactuals should be provided <span class="citation" data-cites="mothilal2020explaining">(<a href="#ref-mothilal2020explaining" role="doc-biblioref">Mothilal, Sharma, and Tan 2020</a>)</span>; and <strong>causality</strong> — counterfactuals should respect the structural causal model underlying the data generating process <span class="citation" data-cites="karimi2020algorithmic karimi2021algorithmic">(<a href="#ref-karimi2020algorithmic" role="doc-biblioref">Karimi, Von Kügelgen, et al. 2020</a>; <a href="#ref-karimi2021algorithmic" role="doc-biblioref">Karimi, Schölkopf, and Valera 2021</a>)</span>.</p>
<p>Beyond gradient-based counterfactual search, which has been the main focus in our development so far, various methodologies have been proposed that can handle non-differentiable models like decision trees. We have implemented some of these approaches and will discuss them further in <a href="#sec-gen">Section&nbsp;3.2</a>.</p>
</section>
<section id="existing-software" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="existing-software"><span class="header-section-number">2.3</span> Existing software</h2>
<p>To the best of our knowledge, the package introduced here provides the first implementation of Counterfactual Explanations in Julia and therefore represents a novel contribution to the community. As for other programming languages, we are only aware of one other unifying framework: the Python library <a href="https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/?badge=latest">CARLA</a> <span class="citation" data-cites="pawelczyk2021carla">(<a href="#ref-pawelczyk2021carla" role="doc-biblioref">Pawelczyk et al. 2021</a>)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> In addition to that, there exists open-source code for some specific approaches to CE that have been proposed in recent years. The approach-specific implementations that we have been able to find are generally well-documented, but exclusively in Python. For example, a PyTorch implementation of a greedy generator for Bayesian models proposed in <span class="citation" data-cites="schut2021generating">Schut et al. (<a href="#ref-schut2021generating" role="doc-biblioref">2021</a>)</span> has been released. As another example, the popular <a href="https://github.com/interpretml">InterpretML</a> library includes an implementation of a diverse counterfactual generator <span class="citation" data-cites="mothilal2020explaining">(<a href="#ref-mothilal2020explaining" role="doc-biblioref">Mothilal, Sharma, and Tan 2020</a>)</span>.</p>
<p>Generally speaking, software development in the space of XAI has largely focused on various global methods and surrogate explainers: implementations of PDP, LIME and SHAP are available for both Python (e.g.&nbsp;<a href="https://github.com/marcotcr/lime"><code>lime</code></a>, <a href="https://github.com/slundberg/shap"><code>shap</code></a>) and R (e.g.&nbsp;<a href="https://cran.r-project.org/web/packages/lime/index.html"><code>lime</code></a>, <a href="https://cran.r-project.org/web/packages/lime/index.html"><code>iml</code></a>, <a href="https://modeloriented.github.io/shapper/"><code>shapper</code></a>, <a href="https://github.com/bgreenwell/fastshap"><code>fastshap</code></a>). In the Julia space, there exist two packages related to XAI: firstly, <a href="https://github.com/nredell/ShapML.jl"><code>ShapML.jl</code></a>, which provides a fast implementation of SHAP; and, secondly, <a href="https://github.com/adrhill/ExplainableAI.jl"><code>ExplainableAI.jl</code></a>, which enables users to easily visualise gradients and activation maps for <code>Flux.jl</code> models. We also should not fail to mention the comprehensive <a href="https://docs.interpretable.ai/stable/IAIBase/data/">Interpretable AI</a> infrastructure, which focuses exclusively on interpretable models.</p>
<p>Arguably the current availability of tools for explaining black-box models in Julia is limited, but it appears that the community is invested in changing that. The team behind <code>MLJ.jl</code>, for example, recruited contributors for a project about both Interpretable and Explainable AI in 2022.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> With our work on Counterfactual Explanations we hope to contribute to these efforts. We think that because of its unique transparency the Julia language naturally lends itself towards building Trustworthy AI systems.</p>
</section>
</section>
<section id="sec-arch" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Introducing: <code>CounterfactualExplanations.jl</code></h1>
<p><a href="#fig-arch">Figure&nbsp;1</a> provides an overview of the package architecture. It is built around two core modules that are designed to be as extensible as possible through dispatch: 1) <code>Models</code> is concerned with making any arbitrary model compatible with the package; 2) <code>Generators</code> is used to implement counterfactual search algorithms. The core function of the package—<code>generate_counterfactual</code>—uses an instance of type <code>&lt;:AbstractFittedModel</code> produced by the <code>Models</code> module and an instance of type <code>&lt;:AbstractGenerator</code> produced by the <code>Generators</code> module. Relating this to the methodology outlined in <a href="#sec-method">Section&nbsp;2.2</a>, the former instance corresponds to the model <span class="math inline">\(M\)</span>, while the latter defines the rules for the counterfactual search (<a href="#eq-solution">Equation&nbsp;2</a>).</p>
<div id="fig-arch" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/pkg_architecture.png" style="width:3.33333in;height:2.38095in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: High-level schematic overview of package architecture. Modules are shown in red, structs in green and functions in purple.</figcaption>
</figure>
</div>
<section id="models" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="models"><span class="header-section-number">3.1</span> Models</h2>
<p>The package currently offers native support for models built and trained in <a href="https://fluxml.ai/">Flux</a> <span class="citation" data-cites="innes2018flux">(<a href="#ref-innes2018flux" role="doc-biblioref">Innes 2018</a>)</span> as well as a small subset of models made available through <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/">MLJ</a> <span class="citation" data-cites="blaom2020mlj">(<a href="#ref-blaom2020mlj" role="doc-biblioref">Blaom et al. 2020</a>)</span>. While in general it is assumed that users resort to this package to explain their pre-trained models, we provide a simple API call to train the following <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/v0.1/tutorials/model_catalogue/">models</a>:</p>
<ul>
<li>Linear Classifier (Logistic Regression and Multinomial Logit)</li>
<li>Multi-Layer Perceptron (Deep Neural Network)</li>
<li>Deep Ensemble <span class="citation" data-cites="lakshminarayanan2016simple">Lakshminarayanan, Pritzel, and Blundell (<a href="#ref-lakshminarayanan2016simple" role="doc-biblioref">2016</a>)</span></li>
<li>Decision Tree, Random Forest, Gradient Boosted Trees</li>
</ul>
<p>As we demonstrate below, it is straightforward to extend the package through custom models. Support for <code>torch</code> models trained in Python or R is also available.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
</section>
<section id="sec-gen" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-gen"><span class="header-section-number">3.2</span> Generators</h2>
<p>A large and growing number of counterfactual <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/v0.1/explanation/generators/overview/">generators</a> have already been implemented in our package (<a href="#tbl-gen">Table&nbsp;1</a>). At a high level, we distinguish generators in terms of their compatible model types, their default search space, and their composability. All “gradient-based” generators are compatible with differentiable models, e.g.&nbsp;<code>Flux</code> and <code>torch</code>, while “tree-based” generators are only applicable to models that involve decision trees. Concerning the search space, it is possible to search counterfactuals in a lower-dimensional latent embedding of the feature space that implicitly encodes the data-generating process (DGP). To learn the latent embedding, existing work has typically relied on generative models or existing causal knowledge <span class="citation" data-cites="joshi2019realistic karimi2021algorithmic">(<a href="#ref-joshi2019realistic" role="doc-biblioref">Joshi et al. 2019</a>; <a href="#ref-karimi2021algorithmic" role="doc-biblioref">Karimi, Schölkopf, and Valera 2021</a>)</span>. While this notion is compatible with all of our gradient-based generators, only some generators search a latent space by default. Finally, composability implies that the given generator can be blended with any other composable generator, which we discuss in <a href="#sec-gen-comp">Section&nbsp;4.2</a>.</p>
<p>Beyond these broad technical distinctions, generators largely differ in terms of how they address the various desiderata mentioned above: <em>ClapROAR</em> aims to preserve the classifier, i.e.&nbsp;to generate counterfactuals that are robust to endogenous model shifts <span class="citation" data-cites="altmeyer2023endogenous">(<a href="#ref-altmeyer2023endogenous" role="doc-biblioref">Altmeyer et al. 2023</a>)</span>; <em>CLUE</em> searches plausible counterfactuals in the latent embedding of a generative model by explicitly minimising predictive entropy <span class="citation" data-cites="antoran2020getting">(<a href="#ref-antoran2020getting" role="doc-biblioref">Antorán et al. 2020</a>)</span>; <em>DiCE</em> is designed to generate multiple, maximally diverse counterfactuals <span class="citation" data-cites="mothilal2020explaining">(<a href="#ref-mothilal2020explaining" role="doc-biblioref">Mothilal, Sharma, and Tan 2020</a>)</span>; <em>FeatureTweak</em> leverages the internals of decision trees to search counterfactuals on a feature-by-feature basis, finding the counterfactual that tweaks the features in the least costly way <span class="citation" data-cites="tolomei2017interpretable">(<a href="#ref-tolomei2017interpretable" role="doc-biblioref">Tolomei et al. 2017</a>)</span>; <em>Gravitational</em> aims to generate plausible and robust counterfactuals by minimising the distance to observed samples in the target class <span class="citation" data-cites="altmeyer2023endogenous">(<a href="#ref-altmeyer2023endogenous" role="doc-biblioref">Altmeyer et al. 2023</a>)</span>; <em>Greedy</em> aims to generate plausible counterfactuals by implicitly minimising predictive uncertainty of Bayesian classifiers <span class="citation" data-cites="schut2021generating">(<a href="#ref-schut2021generating" role="doc-biblioref">Schut et al. 2021</a>)</span>; <em>GrowingSpheres</em> is model-agnostic, relying solely on identifying nearest neighbours of counterfactuals in the target class by gradually increasing the search radius and then moving counterfactuals in that direction<span class="citation" data-cites="laugel2017inversea">(<a href="#ref-laugel2017inversea" role="doc-biblioref">Laugel et al. 2017</a>)</span>; <em>PROBE</em> generates probabilistically robust counterfactuals <span class="citation" data-cites="pawelczyk2022probabilistically">(<a href="#ref-pawelczyk2022probabilistically" role="doc-biblioref">Pawelczyk et al. 2022</a>)</span>; <em>REVISE</em> addresses the need for plausibility by searching counterfactuals in the latent embedding of a Variational Autoencoder (VAE) <span class="citation" data-cites="joshi2019realistic">(<a href="#ref-joshi2019realistic" role="doc-biblioref">Joshi et al. 2019</a>)</span>; <em>Wachter</em> is the baseline approach that only penalises the distance to the original sample <span class="citation" data-cites="wachter2017counterfactual">(<a href="#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span>.</p>
<div id="tbl-gen" class="anchored">
<table class="table">
<caption>Table&nbsp;1: Overview of implemented counterfactual generators.</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Generator</th>
<th>Model Type</th>
<th>Search Space</th>
<th>Composable</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ClaPROAR <span class="citation" data-cites="altmeyer2023endogenous">(<a href="#ref-altmeyer2023endogenous" role="doc-biblioref">Altmeyer et al. 2023</a>)</span></td>
<td>gradient based</td>
<td>feature</td>
<td>yes</td>
</tr>
<tr class="even">
<td>CLUE <span class="citation" data-cites="antoran2020getting">(<a href="#ref-antoran2020getting" role="doc-biblioref">Antorán et al. 2020</a>)</span></td>
<td>gradient based</td>
<td>latent</td>
<td>yes</td>
</tr>
<tr class="odd">
<td>DiCE <span class="citation" data-cites="mothilal2020explaining">(<a href="#ref-mothilal2020explaining" role="doc-biblioref">Mothilal, Sharma, and Tan 2020</a>)</span></td>
<td>gradient based</td>
<td>feature</td>
<td>yes</td>
</tr>
<tr class="even">
<td>FeatureTweak <span class="citation" data-cites="tolomei2017interpretable">(<a href="#ref-tolomei2017interpretable" role="doc-biblioref">Tolomei et al. 2017</a>)</span></td>
<td>tree based</td>
<td>feature</td>
<td>no</td>
</tr>
<tr class="odd">
<td>Gravitational <span class="citation" data-cites="altmeyer2023endogenous">(<a href="#ref-altmeyer2023endogenous" role="doc-biblioref">Altmeyer et al. 2023</a>)</span></td>
<td>gradient based</td>
<td>feature</td>
<td>yes</td>
</tr>
<tr class="even">
<td>Greedy <span class="citation" data-cites="schut2021generating">(<a href="#ref-schut2021generating" role="doc-biblioref">Schut et al. 2021</a>)</span></td>
<td>gradient based</td>
<td>feature</td>
<td>yes</td>
</tr>
<tr class="odd">
<td>GrowingSpheres <span class="citation" data-cites="laugel2017inversea">(<a href="#ref-laugel2017inversea" role="doc-biblioref">Laugel et al. 2017</a>)</span></td>
<td>agnostic</td>
<td>feature</td>
<td>no</td>
</tr>
<tr class="even">
<td>PROBE <span class="citation" data-cites="pawelczyk2022probabilistically">(<a href="#ref-pawelczyk2022probabilistically" role="doc-biblioref">Pawelczyk et al. 2022</a>)</span></td>
<td>gradient based</td>
<td>feature</td>
<td>no</td>
</tr>
<tr class="odd">
<td>REVISE <span class="citation" data-cites="joshi2019realistic">(<a href="#ref-joshi2019realistic" role="doc-biblioref">Joshi et al. 2019</a>)</span></td>
<td>gradient based</td>
<td>latent</td>
<td>yes</td>
</tr>
<tr class="even">
<td>Wachter <span class="citation" data-cites="wachter2017counterfactual">(<a href="#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span></td>
<td>gradient based</td>
<td>feature</td>
<td>yes</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="data-catalogue" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="data-catalogue"><span class="header-section-number">3.3</span> Data Catalogue</h2>
<p>To allow researchers and practitioners to test and compare counterfactual generators, the package ships with <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/v0.1/tutorials/data_catalogue/">catalogues</a> of pre-processed synthetic and real-world benchmark datasets from different domains. Real-world datasets include:</p>
<ul>
<li>Adult Census <span class="citation" data-cites="becker1996adult">(<a href="#ref-becker1996adult" role="doc-biblioref">Barry Becker 1996</a>)</span></li>
<li>California Housing <span class="citation" data-cites="pace1997sparse">(<a href="#ref-pace1997sparse" role="doc-biblioref">Pace and Barry 1997</a>)</span></li>
<li>CIFAR10 <span class="citation" data-cites="krizhevsky2009learning">(<a href="#ref-krizhevsky2009learning" role="doc-biblioref">Krizhevsky 2009</a>)</span></li>
<li>German Credit <span class="citation" data-cites="hoffman1994german">(<a href="#ref-hoffman1994german" role="doc-biblioref">Hoffman 1994</a>)</span></li>
<li>Give Me Some Credit <span class="citation" data-cites="kaggle2011give">(<a href="#ref-kaggle2011give" role="doc-biblioref">Kaggle 2011</a>)</span></li>
<li>MNIST <span class="citation" data-cites="lecun1998mnist">(<a href="#ref-lecun1998mnist" role="doc-biblioref">LeCun 1998</a>)</span> and Fashion MNIST <span class="citation" data-cites="xiao2017fashion">(<a href="#ref-xiao2017fashion" role="doc-biblioref">Xiao, Rasul, and Vollgraf 2017</a>)</span></li>
<li>UCI defaultCredit <span class="citation" data-cites="yeh2009comparisons">(<a href="#ref-yeh2009comparisons" role="doc-biblioref">Yeh and Lien 2009</a>)</span></li>
</ul>
<p>Custom datasets can also be easily preprocessed as explained in the <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/v0.1/tutorials/data_preprocessing/">documentation</a>.</p>
</section>
<section id="plotting" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="plotting"><span class="header-section-number">3.4</span> Plotting</h2>
<p>The package also extends common <code>Plots.jl</code> methods to facilitate the visualization of results. Calling the generic <code>plot()</code> method on an instance of type <code>&lt;:CounterfactualExplanation</code>, for example, generates a plot visualizing the entire counterfactual path in the feature space<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. We will see several examples of this below.</p>
</section>
</section>
<section id="sec-use" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Basic Usage</h1>
<p>In the following, we begin our exploration of the package functionality with a simple example. We then demonstrate how more advanced generators can be easily composed and show how users can impose mutability constraints on features. Finally, we also briefly explore the topics of counterfactual evaluation and benchmarking.</p>
<section id="sec-simple" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-simple"><span class="header-section-number">4.1</span> A Simple Generic Generator</h2>
<p>Code <span class="math inline">\(\ref{lst:simple}\)</span> below provides a complete example demonstrating how the framework presented in <a href="#sec-method">Section&nbsp;2.2</a> can be implemented through our package. Using a synthetic data set with linearly separable features we first fit a linear classifier (line <span class="math inline">\(\ref{line:simple-class}\)</span>). Next, we define the target class (line <span class="math inline">\(\ref{line:simple-t}\)</span>) and then draw a random sample from the other class (line <span class="math inline">\(\ref{line:simple-x}\)</span>). Finally, we instantiate a generic generator (line <span class="math inline">\(\ref{line:simple-gen}\)</span>) and run the counterfactual search (line <span class="math inline">\(\ref{line:simple-search}\)</span>). <a href="#fig-binary">Figure&nbsp;2</a> illustrates the resulting counterfactual path in the two-dimensional feature space. Features go through iterative perturbations until the desired confidence level is reached as illustrated by the contour in the background, which shows the softmax output for the target class.</p>
<div id="fig-binary" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/ce_binary.png" style="width:3.33333in;height:2.5in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Counterfactual path using generic counterfactual generator for conventional binary classifier.</figcaption>
</figure>
</div>
<p>In this simple example, the generic generator produces a valid counterfactual, since the decision boundary is crossed and the predicted label is flipped. But the counterfactual is not plausible: it does not appear to be generated by the same DGP as the observed data in the target class. This is because the generic generator does not take into account any of the desiderata mentioned in <a href="#sec-method">Section&nbsp;2.2</a>, except for the distance to the factual sample.</p>
</section>
<section id="sec-gen-comp" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-gen-comp"><span class="header-section-number">4.2</span> Composing Generators</h2>
<p>To address these issues, we can leverage the ideas underlying some of the more advanced counterfactual generators introduced above. In particular, we will now show how easy it is to <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/v0.1/tutorials/generators/">compose custom generators</a> that blend different ideas through user-friendly macros.</p>
<p>Suppose we wanted to address the desiderata of plausibility and diversity. We could do this by blending ideas underlying the <em>DiCE</em> generator with the <em>REVISE</em> generator. Formally, the corresponding search objective would be defined as follows,</p>
<p><span id="eq-comp"><span class="math display">\[
\mathbf{Z}^\prime = \arg \min_{\mathbf{Z}^\prime \in \mathcal{Z}^{L \times K}} \{  {\ell(M(f(\mathbf{Z}^\prime)),t)} + \lambda \cdot {\text{diversity}(f(\mathbf{Z}^\prime)) }  \}
\tag{3}\]</span></span></p>
<p>where <span class="math inline">\(\mathbf{X}^\prime\)</span> is an <span class="math inline">\(L\)</span>-dimensional array of counterfactuals, <span class="math inline">\(f: \mathcal{Z}^{L \times K} \mapsto \mathcal{X}^{L \times D}\)</span> is a function that maps the <span class="math inline">\(L \times K\)</span>-dimensional latent space <span class="math inline">\(\mathcal{Z}\)</span> to the <span class="math inline">\(L \times D\)</span>-dimensional feature space <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\text{diversity}(\cdot)\)</span> is the penalty proposed by <span class="citation" data-cites="mothilal2020explaining">Mothilal, Sharma, and Tan (<a href="#ref-mothilal2020explaining" role="doc-biblioref">2020</a>)</span> that induces diverse sets of counterfactuals. As in <a href="#eq-solution">Equation&nbsp;2</a>, <span class="math inline">\(\ell\)</span> is the loss function, <span class="math inline">\(M\)</span> is the black-box model, <span class="math inline">\(t\)</span> is the target class, and <span class="math inline">\(\lambda\)</span> is the strength of the penalty.</p>
<p>Code <span class="math inline">\(\ref{lst:composed}\)</span> demonstrates how <a href="#eq-comp">Equation&nbsp;3</a> can be seamlessly translated into Julia code. We begin by instantiating a <code>GradientBasedGenerator</code> in line <span class="math inline">\(\ref{line:composed-init}\)</span>. Next, we use chained macros for composition: firstly, we define the counterfactual search <code>@objective</code> corresponding to <em>DiCE</em> in line <span class="math inline">\(\ref{line:composed-dice}\)</span>; secondly, we define the latent space search strategy corresponding to <em>REVISE</em> using the <code>@search_latent_space</code> macro in line <span class="math inline">\(\ref{line:composed-latent}\)</span>; finally, we specify our prefered optimisation method using the <code>@with_optimiser</code> macro in line <span class="math inline">\(\ref{line:composed-adam}\)</span>.</p>
<p>In this case, the counterfactual search is performed in the latent space of a Variational Autoencoder (VAE) that is automatically trained on the observed data. It is important to specify the keyword argument <code>num_counterfactuals</code> of the <code>generate_counterfactual</code> to some value higher than <span class="math inline">\(1\)</span> (default), to ensure that the diversity penalty is effective. The resulting counterfactual path is shown in <a href="#fig-binary-advanced">Figure&nbsp;3</a> below. We observe that the resulting counterfactuals are diverse and the majority of them are plausible.</p>
<div id="fig-binary-advanced" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/binary_advanced.png" style="width:3.33333in;height:2.5in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Counterfactual path using the <em>DiCE</em> generator.</figcaption>
</figure>
</div>
</section>
<section id="sec-mut" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-mut"><span class="header-section-number">4.3</span> Mutability Constraints</h2>
<p>In practice, features usually cannot be perturbed arbitrarily. Suppose, for example, that one of the features used by a bank to predict the creditworthiness of its clients is <em>age</em>. If a counterfactual explanation for the prediction model indicates that older clients should “grow younger” to improve their creditworthiness, then this is an interesting insight (it reveals age bias), but the provided recourse is not actionable. In such cases, we may want to constrain the mutability of features. To illustrate how this can be implemented in our package, we will continue with the example from above.</p>
<p>Mutability can be defined in terms of four different options: 1) the feature is mutable in both directions, 2) the feature can only increase (e.g.&nbsp;<em>age</em>), 3) the feature can only decrease (e.g.&nbsp;<em>time left</em> until your next deadline) and 4) the feature is not mutable (e.g.&nbsp;<em>skin colour</em>, <em>ethnicity</em>, …). To specify which category a feature belongs to, users can pass a vector of symbols containing the mutability constraints at the pre-processing stage. For each feature one can choose from these four options: <code>:both</code> (mutable in both directions), <code>:increase</code> (only up), <code>:decrease</code> (only down) and <code>:none</code> (immutable). By default, <code>nothing</code> is passed to that keyword argument and it is assumed that all features are mutable in both directions.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>We can impose that the first feature is immutable as follows: <code>counterfactual_data.mutability = [:none, :both]</code>. The resulting counterfactual path is shown in <a href="#fig-mutability">Figure&nbsp;4</a> below. Since only the second feature can be perturbed, the sample can only move along the vertical axis. In this case, the counterfactual search does not yield a valid counterfactual, since the target class is not reached.</p>
<div id="fig-mutability" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/constraint_mutability.png" style="width:3.33333in;height:2.5in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Counterfactual path with immutable feature.</figcaption>
</figure>
</div>
</section>
<section id="sec-eval" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="sec-eval"><span class="header-section-number">4.4</span> Evaluation and Benchmarking</h2>
<p>The package also makes it easy to <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/v0.1/tutorials/evaluation/">evaluate</a> counterfactuals with respect to many of the desiderata mentioned above. For example, users may want to infer how costly the provided recourse is to individuals. To this end, we can measure the distance of the counterfactual from its original value. The API call to compute the distance metric defined in <span class="citation" data-cites="wachter2017counterfactual">Wachter, Mittelstadt, and Russell (<a href="#ref-wachter2017counterfactual" role="doc-biblioref">2017</a>)</span>, for instance, is as simple as <code>evaluate(ce; measure=distance_mad)</code>, where <code>ce</code> can also be a vector of <code>CounterfactualExplanation</code>s.</p>
<p>Additionally, the package provides a <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/v0.1/tutorials/benchmarking/">benchmarking</a> framework that allows users to compare the performance of different generators on a given dataset. In <a href="#fig-bmk">Figure&nbsp;5</a> we show the results of a benchmark comparing several generators in terms of the average cost and implausibility of the generated counterfactuals. The cost is proxied by the L1-norm of the difference between the factual and counterfactual features, while implausibility is measured by the distance of the counterfactuals from samples in the target class. The results illustrate that there is a tradeoff between minimizing costs to individuals and generating plausible counterfactuals.</p>
<div id="fig-bmk" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/bmk.png" style="width:3.33333in;height:2.5in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Benchmarking results for different generators.</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-custom" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Customization and Extensibility</h1>
<p>One of our priorities has been to make our package customizable and extensible. In the long term, we aim to add support for more default models and counterfactual generators. In the short term, it is designed to allow users to integrate models and generators themselves. These community efforts will facilitate our long-term goals.</p>
<section id="sec-custom-mod" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-custom-mod"><span class="header-section-number">5.1</span> Adding Custom Models</h2>
<p>At the high level, only two steps are necessary to make any supervised learning model compatible with our package:</p>
<ul>
<li>: We need to subtype the .</li>
<li>: The functions and need to be extended through custom methods for the model in question.</li>
</ul>
<p>To demonstrate how this can be done in practice, we will reiterate here how native support for <a href="https://fluxml.ai/"><code>Flux.jl</code></a> <span class="citation" data-cites="innes2018flux">(<a href="#ref-innes2018flux" role="doc-biblioref">Innes 2018</a>)</span> deep learning models was enabled.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> Once again we use synthetic data for an illustrative example. Code <span class="math inline">\(\ref{lst:nn}\)</span> below builds a simple model architecture that can be used for a multi-class prediction task. Note how outputs from the final layer are not passed through a softmax activation function, since the counterfactual loss is evaluated with respect to logits as we discussed earlier. The model is trained with dropout.</p>
<p>Code <span class="math inline">\(\ref{lst:mymodel}\)</span> below implements the two steps that were necessary to make Flux models compatible with the package. In line <span class="math inline">\(\ref{line:mymodel-subtype}\)</span> we declare our new struct as a subtype of <code>AbstractDifferentiableModel</code>, which itself is an abstract subtype of <code>AbstractFittedModel</code>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Computing logits amounts to just calling the model on inputs. Predicted probabilities for labels can be computed by passing logits through the softmax function.</p>
<p>The API call for generating counterfactuals for our new model is the same as before. <a href="#fig-multi">Figure&nbsp;6</a> shows the resulting counterfactual path for a randomly chosen sample. In this case, the contour shows the predicted probability that the input is in the target class (<span class="math inline">\(t=2\)</span>).</p>
<div id="fig-multi" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/ce_multi.png" style="width:3.33333in;height:2.5in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Counterfactual path using generic counterfactual generator for multi-class classifier.</figcaption>
</figure>
</div>
</section>
<section id="sec-custom-gen" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-custom-gen"><span class="header-section-number">5.2</span> Adding Custom Generators</h2>
<p>In some cases, composability may not be sufficient to implement specific logics underlying certain counterfactual generators. In such cases, users may want to implement custom generators. To illustrate how this can be done we will consider a simple extension of our <code>GenericGenerator</code>. As we have seen above, Counterfactual Explanations are not unique. In light of this, we might be interested in quantifying the uncertainty around the generated counterfactuals <span class="citation" data-cites="delaney2021uncertainty">(<a href="#ref-delaney2021uncertainty" role="doc-biblioref">Delaney, Greene, and Keane 2021</a>)</span>. One idea could be, to use dropout to randomly switch features on and off in each iteration. Without dwelling further on the merit of this idea, we will now briefly show how this can be implemented.</p>
<section id="a-generator-with-dropout" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="a-generator-with-dropout"><span class="header-section-number">5.2.1</span> A Generator with Dropout</h3>
<p>Code <span class="math inline">\(\ref{lst:dropout}\)</span> below implements two important steps: 1) create an abstract subtype of the <code>AbstractGradientBasedGenerator</code> and 2) create a constructor with an additional field for the dropout probability.</p>
<p>Next, in Code <span class="math inline">\(\ref{lst:generate}\)</span> we define how feature perturbations are generated for our custom dropout generator: in particular, we extend the relevant function through a method that implements the dropout logic.</p>
<p>Finally, we proceed to generate counterfactuals in the same way we always do. The resulting counterfactual path is shown in <a href="#fig-dropout">Figure&nbsp;7</a>.</p>
<div id="fig-dropout" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/dropout.png" style="width:3.33333in;height:2.5in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Counterfactual path for a generator with dropout.</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="sec-emp" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> A Real-World Examples</h1>
<p>Now that we have explained the basic functionality of <code>CounterfactualExplanations.jl</code> through some synthetic examples, it is time to work through examples involving real-world data.</p>
<section id="give-me-some-credit" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="give-me-some-credit"><span class="header-section-number">6.1</span> Give Me Some Credit</h2>
<p>The <em>Give Me Some Credit</em> dataset is one of the tabular real-world datasets that ship with the package <span class="citation" data-cites="kaggle2011give">(<a href="#ref-kaggle2011give" role="doc-biblioref">Kaggle 2011</a>)</span>. It can be used to train a binary classifier to predict whether a borrower is likely to experience financial difficulties in the next two years. In particular, we have an output variable <span class="math inline">\(y \in \{0=\texttt{no stress},1=\texttt{stress}\}\)</span> and a feature matrix <span class="math inline">\(X\)</span> that includes socio-demographic variables like <code>age</code> and <code>income</code>. A retail bank might use such a classifier to determine if potential borrowers should receive credit or not.</p>
<p>For the classification task, we use a Multi-Layer Perceptron with dropout regularization. Using the Gravitational generator <span class="citation" data-cites="altmeyer2023endogenous">(<a href="#ref-altmeyer2023endogenous" role="doc-biblioref">Altmeyer et al. 2023</a>)</span> we will generate counterfactuals for ten randomly chosen individuals that would be denied credit based on our pre-trained model. Concerning the mutability of features, we only impose that the <code>age</code> cannot be decreased.</p>
<p><a href="#fig-credit">Figure&nbsp;8</a> shows the resulting counterfactuals proposed by Wachter in the two-dimensional feature space spanned by the <code>age</code> and <code>income</code> variables. An increase in income and age is recommended for the majority of individuals, which seems plausible: both age and income are typically positively related to creditworthiness.</p>
<div id="fig-credit" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/credit.png" style="width:3.33333in;height:1.33333in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;8: Give Me Some Credit: counterfactuals for would-be borrowers proposed by the Gravitational Generator.</figcaption>
</figure>
</div>
</section>
<section id="mnist" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="mnist"><span class="header-section-number">6.2</span> MNIST</h2>
<p>For our second example, we will look at image data. The MNIST dataset contains 60,000 training samples of handwritten digits in the form of 28x28 pixel grey-scale images <span class="citation" data-cites="lecun1998mnist">(<a href="#ref-lecun1998mnist" role="doc-biblioref">LeCun 1998</a>)</span>. Each image is associated with a label indicating the digit (0-9) that the image represents. The data makes for an interesting case study of CE because humans have a good idea of what plausible counterfactuals of digits look like. For example, if you were asked to pick up an eraser and turn the digit in the left panel of <a href="#fig-mnist">Figure&nbsp;9</a> into a four (4) you would know exactly what to do: just erase the top part.</p>
<p>On the model side, we will use a simple multi-layer perceptron (MLP). Code <span class="math inline">\(\ref{lst:mnist-setup}\)</span> loads the data and the pre-trained MLP. It also loads two pre-trained Variational Auto-Encoders, which will be used by our counterfactual generator of choice for this task: <em>REVISE</em>.</p>
<p>The proposed counterfactuals are shown in <a href="#fig-mnist">Figure&nbsp;9</a>. In the case in which <em>REVISE</em> has access to an expressive VAE (centre), the result looks convincing: the perturbed image does look like it represents a four (4). In terms of explainability, we may conclude that removing the top part of the handwritten nine (9) leads the black-box model to predict that the perturbed image represents a four (4). We should note, however, that the quality of counterfactuals produced by <em>REVISE</em> hinges on the performance of the underlying generative model, as demonstrated by the result on the right. In this case, <em>REVISE</em> uses a weak VAE and the resulting counterfactual is invalid. In light of this, we recommend using Latent Space search with care.</p>
<div id="fig-mnist" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/mnist_9to4_latent.png" style="width:3.33333in;height:1.11111in" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;9: Counterfactual explanations for MNIST using a Latent Space generator: turning a nine (9) into a four (4).</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-outlook" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Discussion and Outlook</h1>
<p>We believe that this package in its current form offers a valuable contribution to ongoing efforts towards XAI in Julia. That being said, there is significant scope for future developments, which we briefly outline in this final section.</p>
<section id="candidate-models-and-generators" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="candidate-models-and-generators"><span class="header-section-number">7.1</span> Candidate models and generators</h2>
<p>The package supports various models and generators either natively or through minimal augmentation. In future work, we would like to prioritize the addition of further predictive models and generators. Concerning the former, it would be useful to add native support for any supervised models built in <code>MLJ.jl</code>, an extensive Machine Learning framework for Julia <span class="citation" data-cites="blaom2020mlj">(<a href="#ref-blaom2020mlj" role="doc-biblioref">Blaom et al. 2020</a>)</span>. This may also involve adding support for regression models as well as additional non-differentiable models. In terms of counterfactual generators, there is a list of recent methodologies that we would like to implement including MINT <span class="citation" data-cites="karimi2021algorithmic">(<a href="#ref-karimi2021algorithmic" role="doc-biblioref">Karimi, Schölkopf, and Valera 2021</a>)</span>, ROAR <span class="citation" data-cites="upadhyay2021robust">(<a href="#ref-upadhyay2021robust" role="doc-biblioref">Upadhyay, Joshi, and Lakkaraju 2021</a>)</span> and FACE <span class="citation" data-cites="poyiadzi2020face">(<a href="#ref-poyiadzi2020face" role="doc-biblioref">Poyiadzi et al. 2020</a>)</span>.</p>
</section>
<section id="additional-datasets" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="additional-datasets"><span class="header-section-number">7.2</span> Additional datasets</h2>
<p>For benchmarking and testing purposes it will be crucial to add more datasets to our library. We have so far prioritized tabular datasets that have typically been used in the literature on counterfactual explanations including <em>Adult</em>, <em>Give Me Some Credit</em> and <em>German Credit</em> <span class="citation" data-cites="karimi2020survey">(<a href="#ref-karimi2020survey" role="doc-biblioref">Karimi, Barthe, et al. 2020</a>)</span>. There is scope for adding data sources that have so far not been explored much in this context including additional image datasets as well as audio, natural language and time-series data.</p>
</section>
</section>
<section id="sec-conclude" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Concluding remarks</h1>
<p><code>CounterfactualExplanation.jl</code> is a package for generating Counterfactual Explanations and Algorithmic Recourse in Julia. Through various synthetic and real-world examples, we have demonstrated the basic usage of the package as well as its extensibility. The package has already served us in our research to benchmark various methodological approaches to Counterfactual Explanations and Algorithmic Recourse. We therefore strongly believe that it should help other practitioners and researchers in their own efforts towards Trustworthy AI.</p>
<p>We envision this package to one day constitute the go-to place for explaining arbitrary predictive models through an extensive suite of counterfactual generators. As a major next step, we aim to make our library as compatible as possible with the popular <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/"><code>MLJ.jl</code></a> package for machine learning in Julia. We invite the Julia community to contribute to these goals through usage, open challenge and active development.</p>
</section>
<section id="sec-ack" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Acknowledgements</h1>
<p>We are immensely grateful to the group of TU Delft students who contributed huge improvements to this package as part of a university project in 2023: Rauno Arike, Simon Kasdorp, Lauri Kesküll, Mariusz Kicior, Vincent Pikand. We also want to thank the broader Julia community for being welcoming and open and for supporting research contributions like this one. Some of the members of TU Delft were partially funded by ICAI AI for Fintech Research, an ING—TU Delft collaboration.</p>
</section>
<section id="references" class="level1 unnumbered" data-number="10">


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">10 References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-altmeyer2023endogenous" class="csl-entry" role="listitem">
Altmeyer, Patrick, Giovan Angela, Aleksander Buszydlik, Karol Dobiczek, Arie van Deursen, and Cynthia Liem. 2023. <span>“Endogenous <span>Macrodynamics</span> in <span>Algorithmic</span> <span>Recourse</span>.”</span> In <em>First <span>IEEE</span> <span>Conference</span> on <span>Secure</span> and <span>Trustworthy</span> <span>Machine</span> <span>Learning</span></em>. <a href="https://doi.org/10.1109/satml54575.2023.00036">https://doi.org/10.1109/satml54575.2023.00036</a>.
</div>
<div id="ref-antoran2020getting" class="csl-entry" role="listitem">
Antorán, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and José Miguel Hernández-Lobato. 2020. <span>“Getting a Clue: <span>A</span> Method for Explaining Uncertainty Estimates.”</span> <a href="https://arxiv.org/abs/2006.06848">https://arxiv.org/abs/2006.06848</a>.
</div>
<div id="ref-arrieta2020explainable" class="csl-entry" role="listitem">
Arrieta, Alejandro Barredo, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, et al. 2020. <span>“Explainable <span>Artificial Intelligence</span> (<span>XAI</span>): <span>Concepts</span>, Taxonomies, Opportunities and Challenges Toward Responsible <span>AI</span>.”</span> <em>Information Fusion</em> 58: 82–115. <a href="https://doi.org/10.1016/j.inffus.2019.12.012">https://doi.org/10.1016/j.inffus.2019.12.012</a>.
</div>
<div id="ref-becker1996adult" class="csl-entry" role="listitem">
Barry Becker, Ronny Kohavi. 1996. <span>“Adult.”</span> UCI Machine Learning Repository. <a href="https://doi.org/10.24432/C5XW20">https://doi.org/10.24432/C5XW20</a>.
</div>
<div id="ref-blaom2020mlj" class="csl-entry" role="listitem">
Blaom, Anthony D., Franz Kiraly, Thibaut Lienart, Yiannis Simillides, Diego Arenas, and Sebastian J. Vollmer. 2020. <span>“<span>MLJ</span>: <span>A Julia</span> Package for Composable Machine Learning.”</span> <em>Journal of Open Source Software</em> 5 (55): 2704. <a href="https://doi.org/10.21105/joss.02704">https://doi.org/10.21105/joss.02704</a>.
</div>
<div id="ref-borch2022machine" class="csl-entry" role="listitem">
Borch, Christian. 2022. <span>“Machine Learning, Knowledge Risk, and Principal-Agent Problems in Automated Trading.”</span> <em>Technology in Society</em>, 101852. <a href="https://doi.org/10.1016/j.techsoc.2021.101852">https://doi.org/10.1016/j.techsoc.2021.101852</a>.
</div>
<div id="ref-buolamwini2018gender" class="csl-entry" role="listitem">
Buolamwini, Joy, and Timnit Gebru. 2018. <span>“Gender Shades: <span>Intersectional</span> Accuracy Disparities in Commercial Gender Classification.”</span> In <em>Conference on Fairness, Accountability and Transparency</em>, 77–91. <span>PMLR</span>.
</div>
<div id="ref-dandl2023counterfactuals" class="csl-entry" role="listitem">
Dandl, Susanne, Andreas Hofheinz, Martin Binder, Bernd Bischl, and Giuseppe Casalicchio. 2023. <span>“Counterfactuals: <span>An</span> <span>R</span> <span>Package</span> for <span>Counterfactual</span> <span>Explanation</span> <span>Methods</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2304.06569">http://arxiv.org/abs/2304.06569</a>.
</div>
<div id="ref-delaney2021uncertainty" class="csl-entry" role="listitem">
Delaney, Eoin, Derek Greene, and Mark T. Keane. 2021. <span>“Uncertainty <span>Estimation</span> and <span>Out</span>-of-<span>Distribution</span> <span>Detection</span> for <span>Counterfactual</span> <span>Explanations</span>: <span>Pitfalls</span> and <span>Solutions</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2107.09734">http://arxiv.org/abs/2107.09734</a>.
</div>
<div id="ref-fan2020interpretability" class="csl-entry" role="listitem">
Fan, Fenglei, Jinjun Xiong, and Ge Wang. 2020. <span>“On Interpretability of Artificial Neural Networks.”</span> <a href="https://arxiv.org/abs/2001.02522">https://arxiv.org/abs/2001.02522</a>.
</div>
<div id="ref-goodfellow2014explaining" class="csl-entry" role="listitem">
Goodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. <span>“Explaining and Harnessing Adversarial Examples.”</span> <a href="https://arxiv.org/abs/1412.6572">https://arxiv.org/abs/1412.6572</a>.
</div>
<div id="ref-hoffman1994german" class="csl-entry" role="listitem">
Hoffman, Hans. 1994. <span>“German <span>Credit Data</span>.”</span> <a href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)">https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</a>.
</div>
<div id="ref-innes2018flux" class="csl-entry" role="listitem">
Innes, Mike. 2018. <span>“Flux: <span>Elegant</span> Machine Learning with <span>Julia</span>.”</span> <em>Journal of Open Source Software</em> 3 (25): 602. <a href="https://doi.org/10.21105/joss.00602">https://doi.org/10.21105/joss.00602</a>.
</div>
<div id="ref-joshi2019realistic" class="csl-entry" role="listitem">
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. <span>“Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.”</span> <a href="https://arxiv.org/abs/1907.09615">https://arxiv.org/abs/1907.09615</a>.
</div>
<div id="ref-kaggle2011give" class="csl-entry" role="listitem">
Kaggle. 2011. <span>“Give Me Some Credit, <span>Improve</span> on the State of the Art in Credit Scoring by Predicting the Probability That Somebody Will Experience Financial Distress in the Next Two Years.”</span> <span>Kaggle</span>. <a href="https://www.kaggle.com/c/GiveMeSomeCredit">https://www.kaggle.com/c/GiveMeSomeCredit</a>.
</div>
<div id="ref-karimi2020survey" class="csl-entry" role="listitem">
Karimi, Amir-Hossein, Gilles Barthe, Bernhard Schölkopf, and Isabel Valera. 2020. <span>“A Survey of Algorithmic Recourse: Definitions, Formulations, Solutions, and Prospects.”</span> <a href="https://arxiv.org/abs/2010.04050">https://arxiv.org/abs/2010.04050</a>.
</div>
<div id="ref-karimi2021algorithmic" class="csl-entry" role="listitem">
Karimi, Amir-Hossein, Bernhard Schölkopf, and Isabel Valera. 2021. <span>“Algorithmic Recourse: From Counterfactual Explanations to Interventions.”</span> In <em>Proceedings of the 2021 <span>ACM Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em>, 353–62.
</div>
<div id="ref-karimi2020algorithmic" class="csl-entry" role="listitem">
Karimi, Amir-Hossein, Julius Von Kügelgen, Bernhard Schölkopf, and Isabel Valera. 2020. <span>“Algorithmic Recourse Under Imperfect Causal Knowledge: A Probabilistic Approach.”</span> <a href="https://arxiv.org/abs/2006.06831">https://arxiv.org/abs/2006.06831</a>.
</div>
<div id="ref-kaur2020interpreting" class="csl-entry" role="listitem">
Kaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. 2020. <span>“Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning.”</span> In <em>Proceedings of the 2020 <span>CHI</span> Conference on Human Factors in Computing Systems</em>, 1–14. <a href="https://doi.org/10.1145/3313831.3376219">https://doi.org/10.1145/3313831.3376219</a>.
</div>
<div id="ref-krizhevsky2009learning" class="csl-entry" role="listitem">
Krizhevsky, A. 2009. <span>“Learning <span>Multiple</span> <span>Layers</span> of <span>Features</span> from <span>Tiny</span> <span>Images</span>.”</span> In. <a href="https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086">https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086</a>.
</div>
<div id="ref-lakshminarayanan2016simple" class="csl-entry" role="listitem">
Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2016. <span>“Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles.”</span> <a href="https://arxiv.org/abs/1612.01474">https://arxiv.org/abs/1612.01474</a>.
</div>
<div id="ref-laugel2017inversea" class="csl-entry" role="listitem">
Laugel, Thibault, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. 2017. <span>“Inverse <span>Classification</span> for <span>Comparison</span>-Based <span>Interpretability</span> in <span>Machine</span> <span>Learning</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1712.08443">https://doi.org/10.48550/arXiv.1712.08443</a>.
</div>
<div id="ref-lecun1998mnist" class="csl-entry" role="listitem">
LeCun, Yann. 1998. <span>“The <span>MNIST</span> Database of Handwritten Digits.”</span>
</div>
<div id="ref-miller2019explanation" class="csl-entry" role="listitem">
Miller, Tim. 2019. <span>“Explanation in Artificial Intelligence: <span>Insights</span> from the Social Sciences.”</span> <em>Artificial Intelligence</em> 267: 1–38. <a href="https://doi.org/10.1016/j.artint.2018.07.007">https://doi.org/10.1016/j.artint.2018.07.007</a>.
</div>
<div id="ref-molnar2020interpretable" class="csl-entry" role="listitem">
Molnar, Christoph. 2020. <em>Interpretable Machine Learning</em>. <span>Lulu. com</span>.
</div>
<div id="ref-mothilal2020explaining" class="csl-entry" role="listitem">
Mothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. <span>“Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.”</span> In <em>Proceedings of the 2020 <span>Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em>, 607–17. <a href="https://doi.org/10.1145/3351095.3372850">https://doi.org/10.1145/3351095.3372850</a>.
</div>
<div id="ref-oneil2016weapons" class="csl-entry" role="listitem">
O’Neil, Cathy. 2016. <em>Weapons of Math Destruction: <span>How</span> Big Data Increases Inequality and Threatens Democracy</em>. <span>Crown</span>.
</div>
<div id="ref-pace1997sparse" class="csl-entry" role="listitem">
Pace, R Kelley, and Ronald Barry. 1997. <span>“Sparse Spatial Autoregressions.”</span> <em>Statistics &amp; Probability Letters</em> 33 (3): 291–97. <a href="https://doi.org/10.1016/s0167-7152(96)00140-x">https://doi.org/10.1016/s0167-7152(96)00140-x</a>.
</div>
<div id="ref-pawelczyk2021carla" class="csl-entry" role="listitem">
Pawelczyk, Martin, Sascha Bielawski, Johannes van den Heuvel, Tobias Richter, and Gjergji Kasneci. 2021. <span>“Carla: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms.”</span> <a href="https://arxiv.org/abs/2108.00783">https://arxiv.org/abs/2108.00783</a>.
</div>
<div id="ref-pawelczyk2022probabilistically" class="csl-entry" role="listitem">
Pawelczyk, Martin, Teresa Datta, Johannes van-den-Heuvel, Gjergji Kasneci, and Himabindu Lakkaraju. 2022. <span>“Probabilistically <span>Robust</span> <span>Recourse</span>: <span>Navigating</span> the <span>Trade</span>-Offs Between <span>Costs</span> and <span>Robustness</span> in <span>Algorithmic</span> <span>Recourse</span>.”</span> <em>arXiv Preprint arXiv:2203.06768</em>.
</div>
<div id="ref-poyiadzi2020face" class="csl-entry" role="listitem">
Poyiadzi, Rafael, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. 2020. <span>“<span>FACE</span>: <span>Feasible</span> and Actionable Counterfactual Explanations.”</span> In <em>Proceedings of the <span>AAAI</span>/<span>ACM Conference</span> on <span>AI</span>, <span>Ethics</span>, and <span>Society</span></em>, 344–50.
</div>
<div id="ref-rudin2019stop" class="csl-entry" role="listitem">
Rudin, Cynthia. 2019. <span>“Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.”</span> <em>Nature Machine Intelligence</em> 1 (5): 206–15. <a href="https://doi.org/10.1038/s42256-019-0048-x">https://doi.org/10.1038/s42256-019-0048-x</a>.
</div>
<div id="ref-schut2021generating" class="csl-entry" role="listitem">
Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. <span>“Generating <span>Interpretable Counterfactual Explanations By Implicit Minimisation</span> of <span>Epistemic</span> and <span>Aleatoric Uncertainties</span>.”</span> In <em>International <span>Conference</span> on <span>Artificial Intelligence</span> and <span>Statistics</span></em>, 1756–64. <span>PMLR</span>.
</div>
<div id="ref-slack2021counterfactual" class="csl-entry" role="listitem">
Slack, Dylan, Anna Hilgard, Himabindu Lakkaraju, and Sameer Singh. 2021. <span>“Counterfactual Explanations Can Be Manipulated.”</span> <em>Advances in Neural Information Processing Systems</em> 34.
</div>
<div id="ref-slack2020fooling" class="csl-entry" role="listitem">
Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. <span>“Fooling Lime and Shap: <span>Adversarial</span> Attacks on Post Hoc Explanation Methods.”</span> In <em>Proceedings of the <span>AAAI</span>/<span>ACM Conference</span> on <span>AI</span>, <span>Ethics</span>, and <span>Society</span></em>, 180–86.
</div>
<div id="ref-tolomei2017interpretable" class="csl-entry" role="listitem">
Tolomei, Gabriele, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. 2017. <span>“Interpretable <span>Predictions</span> of <span>Tree</span>-Based <span>Ensembles</span> via <span>Actionable</span> <span>Feature</span> <span>Tweaking</span>.”</span> In <em>Proceedings of the 23rd <span>ACM</span> <span>SIGKDD</span> <span>International</span> <span>Conference</span> on <span>Knowledge</span> <span>Discovery</span> and <span>Data</span> <span>Mining</span></em>, 465–74. <a href="https://doi.org/10.1145/3097983.3098039">https://doi.org/10.1145/3097983.3098039</a>.
</div>
<div id="ref-upadhyay2021robust" class="csl-entry" role="listitem">
Upadhyay, Sohini, Shalmali Joshi, and Himabindu Lakkaraju. 2021. <span>“Towards <span>Robust</span> and <span>Reliable Algorithmic Recourse</span>.”</span> <a href="https://arxiv.org/abs/2102.13620">https://arxiv.org/abs/2102.13620</a>.
</div>
<div id="ref-ustun2019actionable" class="csl-entry" role="listitem">
Ustun, Berk, Alexander Spangher, and Yang Liu. 2019. <span>“Actionable Recourse in Linear Classification.”</span> In <em>Proceedings of the <span>Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em>, 10–19. <a href="https://doi.org/10.1145/3287560.3287566">https://doi.org/10.1145/3287560.3287566</a>.
</div>
<div id="ref-varshney2022trustworthy" class="csl-entry" role="listitem">
Varshney, Kush R. 2022. <em>Trustworthy <span>Machine Learning</span></em>. <span>Chappaqua, NY, USA</span>: <span>Independently Published</span>.
</div>
<div id="ref-verma2020counterfactual" class="csl-entry" role="listitem">
Verma, Sahil, John Dickerson, and Keegan Hines. 2020. <span>“Counterfactual Explanations for Machine Learning: <span>A</span> Review.”</span> <a href="https://arxiv.org/abs/2010.10596">https://arxiv.org/abs/2010.10596</a>.
</div>
<div id="ref-wachter2017counterfactual" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: <span>Automated</span> Decisions and the <span>GDPR</span>.”</span> <em>Harv. JL &amp; Tech.</em> 31: 841. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.
</div>
<div id="ref-xiao2017fashion" class="csl-entry" role="listitem">
Xiao, Han, Kashif Rasul, and Roland Vollgraf. 2017. <span>“Fashion-<span>MNIST</span>: A <span>Novel</span> <span>Image</span> <span>Dataset</span> for <span>Benchmarking</span> <span>Machine</span> <span>Learning</span> <span>Algorithms</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1708.07747">https://doi.org/10.48550/arXiv.1708.07747</a>.
</div>
<div id="ref-yeh2009comparisons" class="csl-entry" role="listitem">
Yeh, I-Cheng, and Che-hui Lien. 2009. <span>“The Comparisons of Data Mining Techniques for the Predictive Accuracy of Probability of Default of Credit Card Clients.”</span> <em>Expert Systems with Applications</em> 36 (2): 2473–80. <a href="https://doi.org/10.1016/j.eswa.2007.12.020">https://doi.org/10.1016/j.eswa.2007.12.020</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Implementations of loss functions with respect to logits are often numerically more stable. For example, the <code>logitbinarycrossentropy(ŷ, y)</code> implementation in <code>Flux.Losses</code> (used here) is more stable than the mathematically equivalent <code>binarycrossentropy(ŷ, y)</code>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>While we were writing this paper, the <code>R</code> package <code>counterfactuals</code> was released <span class="citation" data-cites="dandl2023counterfactuals">(<a href="#ref-dandl2023counterfactuals" role="doc-biblioref">Dandl et al. 2023</a>)</span>. The developers seem to also envision a unifying framework, but the project appears to still be in its early stages.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For details, see the Google Summer of Code 2022 project proposal: <a href="https://julialang.org/jsoc/gsoc/MLJ/#interpretable_machine_learning_in_julia">https://julialang.org/jsoc/gsoc/MLJ/#interpretable_machine_learning_in_julia</a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>We are currently relying on <code>PythonCall.jl</code> and <code>RCall.jl</code> and this functionality is still somewhat brittle. Since this is more of an edge case, we may move this feature into its own package in the future.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>For multi-dimensional input data, standard dimensionality reduction techniques are used to compress the data. In this case, the classifier’s decision boundary is approximated through a Nearest Neighbour model. This is still somewhat experimental and will be improved in the future.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Mutability constraints are not yet implemented for Latent Space search.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Flux models are now natively supported by our package and can be instantiated by calling <code>FluxModel()</code>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Note that in line <span class="math inline">\(\ref{line:mymodel-likelihood}\)</span> we also provide a field determining the likelihood. This is optional and only used internally to determine which loss function to use in the counterfactual search. If this field is not provided to the model, the loss function needs to be explicitly supplied to the generator.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>